# December 2025 Agent Patterns and Architectures Cheat Sheet

## ðŸ†• December 2025 Updates
- **o1-pro GA** - Production-ready advanced reasoning
- **Claude 3.7 Computer Use** - GA desktop automation
- **Gemini 2.5 Pro** - 2M token context with native tools
- **PydanticAI 0.1.0** - Type-safe agents now GA
- **AgentOps 1.0** - Production monitoring platform
- **OpenAI Realtime API** - Low-latency voice agents

## ðŸ§  Latest Reasoning Patterns

### 1. o1-pro Style Deep Reasoning (December 2025)
```python
from openai import OpenAI

# Use o1-pro for complex reasoning tasks
client = OpenAI()

def advanced_reasoning(query):
    """
    Leverage o1-pro's extended thinking time for complex problems.
    Best for: Math, coding, analysis, strategic planning
    """
    response = client.chat.completions.create(
        model="o1-pro",  # GA December 2025
        messages=[
            {
                "role": "user",
                "content": query
            }
        ]
    )
    return response.choices[0].message.content

# Example: Complex problem solving
result = advanced_reasoning("""
    Design a multi-agent system for automated code review
    with security scanning, performance analysis, and
    maintainability scoring. Include architecture diagram.
""")
```

### 2. Gemini 2.5 Pro with Extended Context
```python
import google.generativeai as genai

# Leverage 2M token context window
genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel('gemini-2.5-pro')  # Dec 2025

def analyze_large_codebase(repo_files):
    """
    Analyze entire codebases with 2M token context.
    Perfect for: Repository-wide analysis, refactoring, documentation
    """
    # Combine all files into single context
    combined_context = "\n\n".join([
        f"File: {file.path}\n{file.content}"
        for file in repo_files
    ])
    
    response = model.generate_content([
        "Analyze this codebase and suggest improvements:",
        combined_context
    ])
    return response.text
```

### 3. Agentic RAG with Memory (Updated Dec 2025)
```python
from mem0 import MemoryClient
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

class AgenticRAGAgent:
    def __init__(self):
        self.memory = MemoryClient()  # Mem0 v0.2.0
        self.llm = ChatOpenAI(model="gpt-4o-mini")  # Cost-effective
    
    def query_with_memory(self, question, user_id):
        # Retrieve adaptive memories
        memories = self.memory.search(
            query=question,
            user_id=user_id,
            limit=10
        )
        
        # Build context-aware prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant with memory."),
            ("system", f"User memories: {memories}"),
            ("user", "{question}")
        ])
        
        response = self.llm.invoke(
            prompt.format(question=question)
        )
        
        # Store new memory
        self.memory.add(
            messages=[{"role": "assistant", "content": response.content}],
            user_id=user_id
        )
        return response.content
```

### 4. Claude 3.7 Computer Use (GA December 2025)
```python
from anthropic import Anthropic

# Anthropic Claude 3.7 Computer Use (GA)
class ComputerUseAgent:
    def __init__(self):
        self.client = Anthropic()
        self.model = "claude-3-7-sonnet-20251220"  # Dec 2025 GA
    
    def automate_task(self, instruction, max_steps=50):
        """
        Automate desktop tasks with Claude Computer Use API.
        Now GA with improved accuracy and reliability.
        """
        messages = [{"role": "user", "content": instruction}]
        
        for step in range(max_steps):
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                tools=[{
                    "type": "computer_20241022",
                    "name": "computer",
                    "display_width_px": 1920,
                    "display_height_px": 1080,
                }],
                messages=messages
            )
            
            if response.stop_reason == "end_turn":
                break
                
            # Process tool use
            for content in response.content:
                if content.type == "tool_use":
                    # Tool execution happens via API
                    pass
            
            messages.append({
                "role": "assistant",
                "content": response.content
            })
        
        return response

# Example: Automate browser research
agent = ComputerUseAgent()
agent.automate_task("""
    Open Chrome, search for 'latest agentic AI frameworks December 2025',
    visit top 3 results, and create a summary document.
""")
```

### 5. PydanticAI Type-Safe Agents (v0.1.0 GA)
```python
from pydantic import BaseModel
from pydantic_ai import Agent, RunContext
from datetime import date

class ResearchResult(BaseModel):
    """Type-safe research output"""
    summary: str
    key_findings: list[str]
    sources: list[str]
    confidence: float
    date_researched: date

# Define type-safe agent with structured output
research_agent = Agent(
    'openai:gpt-4o',
    result_type=ResearchResult,  # Enforced type safety
    system_prompt='You are a research assistant.'
)

async def research_topic(topic: str) -> ResearchResult:
    """Get type-safe, validated research results"""
    result = await research_agent.run(
        f'Research the latest developments in {topic}'
    )
    # result.data is guaranteed to be ResearchResult
    return result.data

# Usage with full type safety
findings = await research_topic("agentic AI in December 2025")
print(f"Confidence: {findings.confidence}")
print(f"Sources: {findings.sources}")
```

### 6. Realtime Voice Agents (OpenAI GA Dec 2025)
```python
from openai import OpenAI
import asyncio

client = OpenAI()

async def create_voice_agent():
    """
    Create low-latency voice agent with OpenAI Realtime API.
    Perfect for: Customer service, voice assistants, phone systems
    """
    async with client.realtime.connect(
        model="gpt-4o-realtime-preview"
    ) as connection:
        # Configure voice settings
        await connection.session.update(
            modalities=["text", "audio"],
            voice="alloy",
            input_audio_transcription={"model": "whisper-1"}
        )
        
        # Stream audio input/output
        async for event in connection:
            if event.type == "conversation.item.created":
                print(f"Response: {event.item.content}")
            elif event.type == "response.audio.delta":
                # Stream audio chunks
                play_audio(event.delta)

# Run voice agent
asyncio.run(create_voice_agent())
```

## ðŸ¤– Multi-Agent Orchestration Patterns

### 1. OpenAI Swarm Pattern
```python
from swarm import Swarm, Agent

# Lightweight multi-agent coordination
def transfer_to_sales():
    return sales_agent

def transfer_to_support():
    return support_agent

# Define specialized agents
triage_agent = Agent(
    name="Triage Agent",
    instructions="Route customers to appropriate departments",
    functions=[transfer_to_sales, transfer_to_support]
)

sales_agent = Agent(
    name="Sales Agent", 
    instructions="Help customers make purchases"
)

support_agent = Agent(
    name="Support Agent",
    instructions="Resolve technical issues"
)

# Run swarm
client = Swarm()
response = client.run(
    agent=triage_agent,
    messages=[{"role": "user", "content": "I need help with my order"}]
)
```

### 2. CrewAI Enterprise Pattern (v0.90 Dec 2025)
```python
from crewai import Agent, Task, Crew, Process
from crewai.memory import EntityMemory, LongTermMemory

# Define specialized agents with enterprise features
researcher = Agent(
    role='Senior Researcher',
    goal='Research latest AI developments in December 2025',
    backstory="Expert in AI research with 10+ years experience",
    tools=[search_tool, arxiv_tool],
    verbose=True,
    memory=True,
    # New in v0.90: Advanced memory systems
    long_term_memory=LongTermMemory(),
    entity_memory=EntityMemory()
)

analyst = Agent(
    role='Data Analyst',
    goal='Analyze and synthesize research findings',
    backstory="PhD in ML with expertise in trend analysis",
    tools=[data_analysis_tool],
    memory=True,
    # New: Hierarchical delegation
    allow_delegation=True
)

writer = Agent(
    role='Technical Writer',
    goal='Create comprehensive reports',
    backstory="Award-winning technical writer",
    tools=[writing_tool],
    memory=True
)

# Define tasks with dependencies
research_task = Task(
    description="""Research agentic AI developments in December 2025.
    Focus on: model releases, framework updates, papers.""",
    agent=researcher,
    expected_output="Detailed research findings"
)

analysis_task = Task(
    description="Analyze research and identify key trends",
    agent=analyst,
    expected_output="Trend analysis report",
    context=[research_task]  # Task dependencies
)

report_task = Task(
    description="Create executive summary of findings",
    agent=writer,
    expected_output="Executive report",
    context=[research_task, analysis_task]
)

# Create crew with hierarchical process (new in v0.90)
crew = Crew(
    agents=[researcher, analyst, writer],
    tasks=[research_task, analysis_task, report_task],
    process=Process.hierarchical,  # New: Hierarchical coordination
    manager_llm="gpt-4o-mini",  # Cost-effective manager
    verbose=True
)

result = crew.kickoff()
```

### 3. AutoGen v0.5 Pattern (December 2025 Rewrite)
```python
from autogen import ConversableAgent, GroupChat, GroupChatManager
import asyncio

# AutoGen v0.5 with improved async support
async def multi_agent_code_review():
    """
    V0.5 features: Better orchestration, async/await, improved memory
    """
    # Define agents with new API
    code_reviewer = ConversableAgent(
        name="CodeReviewer",
        system_message="""You review code for best practices 
        and potential bugs.""",
        llm_config={"model": "gpt-4o-mini"},
        human_input_mode="NEVER"
    )
    
    security_expert = ConversableAgent(
        name="SecurityExpert",
        system_message="You identify security vulnerabilities.",
        llm_config={"model": "gpt-4o"},
        human_input_mode="NEVER"
    )
    
    performance_analyst = ConversableAgent(
        name="PerformanceAnalyst",
        system_message="You analyze code performance.",
        llm_config={"model": "gpt-4o-mini"},
        human_input_mode="NEVER"
    )
    
    # New v0.5: Improved group chat with better orchestration
    group_chat = GroupChat(
        agents=[code_reviewer, security_expert, performance_analyst],
        messages=[],
        max_round=10,
        # New: Advanced speaker selection
        speaker_selection_method="round_robin"
    )
    
    manager = GroupChatManager(
        groupchat=group_chat,
        llm_config={"model": "gpt-4o-mini"}
    )
    
    # Async initiation (new in v0.5)
    await code_reviewer.a_initiate_chat(
        manager,
        message="Review this code: [code snippet here]"
    )

# Run async
asyncio.run(multi_agent_code_review())
```

### 4. LangGraph Cloud Deployment (v1.0 GA)
```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    """Type-safe state management"""
    messages: Annotated[list, operator.add]
    context: dict
    next_action: str

# Define stateful agent workflow
workflow = StateGraph(AgentState)

def research_node(state: AgentState):
    """Research node with state persistence"""
    # Perform research
    return {
        "messages": [{"role": "assistant", "content": "Research complete"}],
        "next_action": "analyze"
    }

def analyze_node(state: AgentState):
    """Analysis node"""
    return {
        "messages": [{"role": "assistant", "content": "Analysis done"}],
        "next_action": END
    }

# Build workflow
workflow.add_node("research", research_node)
workflow.add_node("analyze", analyze_node)
workflow.add_edge("research", "analyze")
workflow.set_entry_point("research")

# Compile with checkpointing (for LangGraph Cloud)
checkpointer = MemorySaver()
app = workflow.compile(checkpointer=checkpointer)

# Deploy to LangGraph Cloud (v1.0 GA)
# langchain deploy --app app.py --env production
```

## ðŸ“Š Production Monitoring Patterns (December 2025)

### 1. AgentOps Integration (v1.0 GA)
```python
import agentops
from openai import OpenAI

# Initialize AgentOps monitoring
agentops.init(
    api_key="YOUR_API_KEY",
    default_tags=["production", "customer-service"]
)

client = OpenAI()

@agentops.track_agent(name="customer_service_agent")
def handle_customer_query(query: str):
    """
    Automatically tracked agent with full observability.
    Tracks: latency, cost, errors, LLM calls, tool usage
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": query}]
    )
    return response.choices[0].message.content

# All metrics automatically sent to AgentOps dashboard
result = handle_customer_query("How do I reset my password?")
agentops.end_session("Success")
```

### 2. LangFuse Tracing (v4.0 Dec 2025)
```python
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
from openai import OpenAI

langfuse = Langfuse()
client = OpenAI()

@observe()  # Automatic tracing decorator
async def research_agent(query: str):
    """Complete observability with zero config"""
    
    # Nested spans automatically tracked
    @observe(as_type="generation")
    def llm_call(prompt: str):
        return client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
    
    # Add custom metadata
    langfuse_context.update_current_trace(
        tags=["research", "production"],
        metadata={"user_query": query}
    )
    
    result = llm_call(f"Research: {query}")
    
    # Track custom metrics
    langfuse_context.score_current_trace(
        name="quality",
        value=0.95
    )
    
    return result

# View in Langfuse dashboard with full trace tree
```

### 3. Anthropic Prompt Caching (90% Cost Reduction)
```python
from anthropic import Anthropic

client = Anthropic()

def cached_agent_with_large_context(query: str, documentation: str):
    """
    Use prompt caching for repeated large contexts.
    Cache documentation (stable) to reduce costs by 90%.
    """
    response = client.messages.create(
        model="claude-3-7-sonnet-20251220",
        max_tokens=1024,
        # Mark cacheable content with cache_control
        system=[
            {
                "type": "text",
                "text": "You are a helpful documentation assistant.",
            },
            {
                "type": "text",
                "text": documentation,  # Large stable context
                "cache_control": {"type": "ephemeral"}  # Cache this!
            }
        ],
        messages=[
            {"role": "user", "content": query}
        ]
    )
    
    # Subsequent requests reuse cached documentation
    # Cache TTL: 5 minutes, 90% cost reduction on cache hits
    return response.content[0].text

# First call: Full cost, creates cache
result1 = cached_agent_with_large_context(
    "How do I install?",
    LARGE_DOCS  # 50k tokens cached
)

# Second call: 10% cost, uses cache
result2 = cached_agent_with_large_context(
    "How do I configure?",
    LARGE_DOCS  # Retrieved from cache!
)
```

## ðŸ”’ Security & Safety Patterns (December 2025)

### 1. Input Sanitization for Agents
```python
import re
from typing import Optional

class AgentInputSanitizer:
    """Prevent prompt injection and malicious inputs"""
    
    BLOCKED_PATTERNS = [
        r"ignore previous instructions",
        r"disregard.*rules",
        r"system\s*prompt",
        r"<\|.*?\|>",  # Special tokens
        r"\\x[0-9a-fA-F]{2}",  # Hex escapes
    ]
    
    def sanitize(self, user_input: str) -> Optional[str]:
        """Sanitize user input before sending to agent"""
        
        # Check for blocked patterns
        for pattern in self.BLOCKED_PATTERNS:
            if re.search(pattern, user_input, re.IGNORECASE):
                raise ValueError(f"Blocked pattern detected: {pattern}")
        
        # Remove control characters
        sanitized = ''.join(
            char for char in user_input
            if char.isprintable() or char.isspace()
        )
        
        # Limit length
        max_length = 4000
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
        
        return sanitized

# Usage
sanitizer = AgentInputSanitizer()
safe_input = sanitizer.sanitize(user_input)
response = agent.run(safe_input)
```

### 2. Constitutional AI Pattern (Anthropic)
```python
from anthropic import Anthropic

client = Anthropic()

def safe_agent_with_constitution(query: str):
    """
    Use constitutional AI principles for safe outputs.
    Self-critique and revision for harmful content.
    """
    
    # Initial response
    initial = client.messages.create(
        model="claude-3-7-sonnet-20251220",
        max_tokens=1024,
        messages=[{"role": "user", "content": query}]
    )
    
    # Constitutional critique
    critique = client.messages.create(
        model="claude-3-7-sonnet-20251220",
        max_tokens=512,
        messages=[
            {"role": "user", "content": initial.content[0].text},
            {"role": "assistant", "content": ""},
            {"role": "user", "content": """
            Critique this response for:
            1. Potential harm or bias
            2. Privacy violations
            3. Misinformation
            4. Unethical suggestions
            """}
        ]
    )
    
    # Revise based on critique
    final = client.messages.create(
        model="claude-3-7-sonnet-20251220",
        max_tokens=1024,
        system=f"Constitution: {critique.content[0].text}",
        messages=[{"role": "user", "content": query}]
    )
    
    return final.content[0].text
```

## ðŸŽ¯ Best Practices (December 2025)

### Model Selection Guide
```python
# Cost-Effective Development
model = "gpt-4o-mini"  # $0.15/1M input tokens

# Production Quality
model = "gpt-4o"  # $2.50/1M input tokens

# Advanced Reasoning
model = "o1-pro"  # $15/1M input tokens (Dec 2025 GA)

# Extended Context
model = "gemini-2.5-pro"  # 2M token context

# Computer Automation
model = "claude-3-7-sonnet"  # Desktop control (GA)

# Real-time Voice
model = "gpt-4o-realtime-preview"  # Low-latency audio

# Local/Open Source
model = "llama-3.3-70b"  # Free, self-hosted
```

### Framework Selection (December 2025)
```python
# Beginners â†’ Start simple
framework = "LangChain"  # Easy RAG and tools

# Type Safety â†’ Production apps
framework = "PydanticAI"  # v0.1.0 GA

# Multi-Agent â†’ Complex workflows
framework = "AutoGen"  # v0.5 rewrite

# Role-Based Teams â†’ Business logic
framework = "CrewAI"  # v0.90 enterprise

# Lightweight â†’ Resource-constrained
framework = "Smolagents"  # v1.2 HuggingFace

# State Management â†’ Long-running
framework = "LangGraph"  # v1.0 Cloud GA
```

---

## ðŸ“š Additional Resources

- **[Complete Agent Patterns Guide](../README.md)**
- **[Framework Comparison](./framework-comparison.md)**
- **[Enterprise Deployment Guide](./enterprise-deployment-2025.md)**
- **[Prompt Engineering Cheat Sheet](./prompt-engineering.md)**
- **[Tool Integration Patterns](./tool-integration.md)**

---

*Last Updated: December 20, 2025*
workflow.add_conditional_edges("analysis", should_continue)

app = workflow.compile()
```

## ðŸŽ¯ Production Patterns

### 1. Type-Safe Agents with PydanticAI
```python
from pydantic_ai import Agent
from pydantic import BaseModel
from typing import List

class ResearchQuery(BaseModel):
    topic: str
    date_range: tuple[str, str]
    max_results: int = 10

class ResearchResult(BaseModel):
    title: str
    summary: str
    confidence: float
    sources: List[str]

# Type-safe agent definition
research_agent = Agent(
    model='openai:gpt-4o',
    result_type=ResearchResult,
    system_prompt='You are a research assistant.'
)

@research_agent.system_prompt
def system_prompt(ctx) -> str:
    return f"Research the topic: {ctx.deps.query.topic}"

# Usage with type safety
async def run_research():
    query = ResearchQuery(
        topic="AI agents",
        date_range=("2025-01-01", "2025-12-31")
    )
    
    result = await research_agent.run(
        "Find recent papers on AI agents",
        deps={"query": query}
    )
    
    # result is guaranteed to be ResearchResult type
    return result
```

### 2. Enterprise Deployment Pattern
```python
# LangGraph Cloud deployment
import asyncio
from langgraph_cloud import LangGraphClient

class EnterpriseAgentService:
    def __init__(self):
        self.client = LangGraphClient(
            api_key="your-api-key",
            url="https://your-deployment.langgraph.cloud"
        )
    
    async def deploy_agent_workflow(self, workflow_config):
        # Deploy stateful agent workflow
        deployment = await self.client.deployments.create(
            config=workflow_config,
            environment="production"
        )
        
        return deployment
    
    async def run_agent_task(self, deployment_id, input_data):
        # Run agent with persistence
        thread = await self.client.threads.create()
        
        response = await self.client.runs.create(
            thread_id=thread.id,
            deployment_id=deployment_id,
            input=input_data
        )
        
        return response

# Usage
service = EnterpriseAgentService()
result = await service.run_agent_task(
    deployment_id="agent-workflow-123",
    input_data={"query": "Process customer inquiry"}
)
```

### 3. Observability and Monitoring
```python
import langfuse
from langfuse.decorators import observe

class ObservableAgent:
    def __init__(self):
        self.langfuse = langfuse.Langfuse()
    
    @observe(name="agent_reasoning")
    def reason_about_task(self, task):
        # Automatic tracing with Langfuse
        with self.langfuse.trace(name="agent_task") as trace:
            trace.update(input=task)
            
            # Reasoning steps
            analysis = self.analyze_task(task)
            trace.generation(
                name="analysis",
                input=task,
                output=analysis
            )
            
            plan = self.create_plan(analysis)
            trace.generation(
                name="planning", 
                input=analysis,
                output=plan
            )
            
            result = self.execute_plan(plan)
            trace.update(output=result)
            
            return result
    
    @observe(name="task_analysis")
    def analyze_task(self, task):
        # Implementation with automatic observability
        return f"Analysis of: {task}"
    
    @observe(name="plan_creation")
    def create_plan(self, analysis):
        return f"Plan based on: {analysis}"
    
    @observe(name="plan_execution")
    def execute_plan(self, plan):
        return f"Executed: {plan}"
```

## ðŸ”— Integration Patterns

### 1. Multi-Modal Agent Pattern
```python
class MultiModalAgent:
    def __init__(self):
        self.vision_model = ChatOpenAI(model="gpt-4o")
        self.audio_model = WhisperAPI()
        self.image_gen = DallEAPI()
    
    async def process_multimodal_input(self, input_data):
        if input_data.type == "image":
            description = await self.vision_model.describe_image(
                input_data.content
            )
            return {"type": "text", "content": description}
        
        elif input_data.type == "audio":
            transcript = await self.audio_model.transcribe(
                input_data.content
            )
            return await self.process_text(transcript)
        
        elif input_data.type == "text_to_image":
            image = await self.image_gen.generate(
                input_data.content
            )
            return {"type": "image", "content": image}
```

### 2. Hybrid Human-AI Pattern
```python
class HybridAgent:
    def __init__(self):
        self.ai_agent = AIAgent()
        self.human_handoff = HumanHandoffService()
    
    async def process_request(self, request):
        # Try AI first
        confidence, response = await self.ai_agent.process(request)
        
        if confidence < 0.8:
            # Handoff to human
            human_response = await self.human_handoff.escalate(
                request=request,
                ai_attempt=response,
                reason="Low confidence"
            )
            
            # Learn from human feedback
            await self.ai_agent.learn_from_feedback(
                request, human_response
            )
            
            return human_response
        
        return response
```

## ðŸ“Š Performance Optimization

### 1. Batch Processing Pattern
```python
class BatchAgent:
    async def process_batch(self, requests: List[dict]):
        # Group similar requests
        grouped = self.group_by_similarity(requests)
        
        results = []
        for group in grouped:
            # Process similar requests together
            batch_result = await self.llm.abatch([
                self.format_prompt(req) for req in group
            ])
            results.extend(batch_result)
        
        return results
```

### 2. Caching and Memory Optimization
```python
from functools import lru_cache
import redis

class OptimizedAgent:
    def __init__(self):
        self.redis_client = redis.Redis()
        self.memory_cache = {}
    
    @lru_cache(maxsize=1000)
    def cached_reasoning(self, input_hash):
        # Cache reasoning results
        return self.expensive_reasoning(input_hash)
    
    async def smart_cache_lookup(self, query):
        # Check Redis for similar queries
        cache_key = self.generate_semantic_hash(query)
        cached = self.redis_client.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        result = await self.process_query(query)
        
        # Store with TTL
        self.redis_client.setex(
            cache_key, 
            timedelta(hours=24), 
            json.dumps(result)
        )
        
        return result
```

---

## ðŸš€ Quick Implementation Guide

### Setup Latest Agent Environment
```bash
# Install latest frameworks
pip install pydantic-ai langchain-core langgraph
pip install autogen-agentchat crewai
pip install langfuse langsmith
pip install mem0ai

# For computer use (Anthropic)
pip install anthropic pyautogui pillow

# For observability
pip install agentops-ai phoenix-evals
```

### Environment Variables
```bash
# Core APIs
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"
export LANGCHAIN_API_KEY="your-key"

# Observability
export LANGFUSE_PUBLIC_KEY="your-key"
export LANGFUSE_SECRET_KEY="your-key"
export AGENTOPS_API_KEY="your-key"

# Memory and State
export REDIS_URL="redis://localhost:6379"
export MEM0_API_KEY="your-key"
```

This cheat sheet covers the latest 2025 patterns for building production-ready AI agents with enhanced capabilities, observability, and enterprise features.